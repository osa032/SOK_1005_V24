# -*- coding: utf-8 -*-
"""scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nXjrZSXvfWmBbg1CqMGVRR_L3WfWb2Ez
"""

#Laster inn pakker
import pandas as pd
import requests
from bs4 import BeautifulSoup
import csv

# Jeg blir å skrape fra http://books.toscrape.com/, som er en nettside laget for å øve seg å scrape data
# Definerer funksjoner for å få hentet data
def scrape_books(url):
    # Det er flere sider, så må hente flere URL'er for å få med alt
    all_books = []

    while True:
        # Ber om HTTP
        try:
            response = requests.get(url)
        except requests.RequestException as e:
            print(e)
            break

        # Henter html innhold
        soup = BeautifulSoup(response.content, 'html.parser')

        # Henter dette
        for listing in soup.find_all("article", class_="product_pod"):
            rating = listing.find('p', class_='star-rating')['class'][1]  # Får med "rating"
            title = listing.find('img')['alt']
            price = listing.find('p', class_="price_color").text
            inStock = "yes" if "In stock" in listing.find('p', class_="instock availability").get_text(strip=True) else "no"
            all_books.append([rating, title, price, inStock])

        # bruker html-scraping for å finne knappen til neste side
        next_button = soup.find("li", class_="next")
        if next_button:
            url = "http://books.toscrape.com/catalogue/" + next_button.find("a")["href"]
        else:
            break  # Når det ikke lengre er en neste knapp så stopper den

    return all_books


def save_books_to_csv(books):
    # Lagrer dataen per siste deloppgave
    with open('data.csv', 'w+', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["rating", "title", "price", "inStock"])
        writer.writerows(books)
    print("Data written to 'data.csv'.")

# Kjører alt
start_url = "http://books.toscrape.com/catalogue/page-1.html"
books = scrape_books(start_url)
save_books_to_csv(books)

# Lagrer data i dataframe
df = pd.DataFrame(books)
df.columns = ["rating", "title", "price", "inStock"]

# Gjør noe artig med dataen, sjekker antall bøker per rating
from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('rating').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Gjør noe annet artig med dataen, sjekker avg pris per rating, og
# Gjør pris om til tall/float
df['price'] = df['price'].replace('[£]', '', regex=True).astype(float)

# Finner gjennomsnittelig pris per rating
avg_price_per_rating = df.groupby('rating')['price'].mean()

# Kategoriserer etter pris
sorted_prices = df['price'].sort_values()
cheap_threshold = sorted_prices.iloc[99]  # 100 billigste
expensive_threshold = sorted_prices.iloc[-100]  # 100 dyreste

# Kategoriserer etter prisene
def categorize_price(price):
    if price <= cheap_threshold:
        return 'Cheap'
    elif price >= expensive_threshold:
        return 'Expensive'
    else:
        return 'Normal'

# Setter sammen
df['price_category'] = df['price'].apply(categorize_price)

# Finer gjennomsnittelig pris per kategori
avg_price_per_category = df.groupby('price_category')['price'].mean()

# Resultater
print("Average Price per Rating:")
print(avg_price_per_rating)
print("\nBooks in each Price Category:")
print(avg_price_per_category)

#Lagrer i df og som csv

avg_price_df = pd.DataFrame(avg_price_per_rating)
price_category_df = pd.DataFrame(avg_price_per_category)

avg_price_df.to_csv('average_prices_per_rating.csv', index=False)
price_category_df.to_csv('books_per_price_category.csv', index=False)

"""KI har blitt brukt for å optimalisere kode"""